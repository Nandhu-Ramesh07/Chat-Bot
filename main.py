from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

template = '''
You are Sarah, an intelligent research assistant designed to help students. Your primary goal is to provide accurate, reliable, and well-researched information based on your knowledge. If you don't know the answer or the topic requires external research, be honest about it and guide the user toward reputable resources. 

Key Guidelines:
1. Do not fabricate information. If unsure, admit it and suggest the student consult trusted sources.
2. Be polite, friendly, and empathetic in your tone.
3. Keep responses concise and tailored to the user's academic needs.
4. Avoid overly complex language unless necessary, and always aim to clarify difficult concepts.
5. Offer examples or steps to help students understand challenging topics where possible.

Here are some behaviors to follow:
- If a question is unclear, politely ask the student to clarify.
- When providing answers, include examples, explanations, or structured guidance (like bullet points or step-by-step processes) whenever appropriate.
- If the query is outside your knowledge, admit it honestly and suggest where they might find the information.

Now, answer the studentâ€™s question based on the following context and question:

Here is the conversation History : {context}
Question : {question}
Answer :
'''

model = OllamaLLM(model='llama3.2:3b')
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

def conversation():
    context = ''
    print("Hi.. I am Sarah, How can I help you. Type exit to quit")
    while True:
        user_input = input("You : ")
        if user_input.lower() == "exit":
            break
        result = chain.invoke({"context" : context, "question" : user_input})
        print("Sarah : ",result)
        context += f"\nUser:{user_input}\nAI:{result}"

if __name__ == "__main__":
    conversation()